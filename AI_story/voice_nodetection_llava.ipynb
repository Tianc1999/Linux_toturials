{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice AI Agent with Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install Import necessary library\n",
    "import cv2\n",
    "import os \n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from yolov8.yolov8 import YOLOv8\n",
    "from yolov8.utils import class_names\n",
    "import pyttsx3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init camera and YOLOv8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init camera\n",
    "video_cap = cv2.VideoCapture(0)\n",
    "video_cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "video_cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Create an inference session using the Vitis AI execution provider\n",
    "#input_model_file = r\"onnx_utils\\DetectionModel_int.onnx\"\n",
    "#config_file_path = r\"onnx_utils\\vaip_config.json\"\n",
    "#model = YOLOv8(input_model_file, config_file_path, 0.25) # probability threshold: 0.25. Notes: high probability threshold will casue the model failing in detecting object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture one image\n",
    "\n",
    "success, frame = video_cap.read()\n",
    "frame = frame[:, 280:-280, ::-1]\n",
    "\n",
    "print(frame.shape)\n",
    "plt.imshow(frame)\n",
    "plt.imsave('temp\\out_for_llava.jpg', frame)\n",
    "\n",
    "#ret = model.detect_objects(frame[:, :, ::-1])\n",
    "#detected_img = model.draw_detections(frame)\n",
    "#plt.imshow(detected_img)\n",
    "#plt.imsave('temp\\\\result.jpg', detected_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parse label\n",
    "# def parse_ret(ret, origin_img):\n",
    "#     parsed_data = []\n",
    "#     parsed_img = {}\n",
    "#     for i in range(len(ret[0])):\n",
    "#         x1, y1, x2, y2 = ret[0][i].astype(np.int32)\n",
    "#         object_dict = {\n",
    "#             \"Index\": \"ThisIsSearchIndex_\"+str(i),\n",
    "#             \"coordination\": [str((x1+x2)//2), str((y1+y2)//2)],\n",
    "#             \"class\": class_names[ret[2][i]],\n",
    "#         }\n",
    "#         parsed_img[\"ThisIsSearchIndex_\"+str(i)] = origin_img[y1:y2, x1:x2, :]\n",
    "#         parsed_data.append(object_dict)\n",
    "#     return parsed_data, parsed_img\n",
    "\n",
    "\n",
    "# parsed_result, parsed_img = parse_ret(ret, frame[:, :, ::1])\n",
    "# print(parsed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTER YOUR QUERY HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"please analyze the picture for me and tell me a story?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This our enhanced prompt here. \n",
    "This prompt is aimed at link the objects in image with the detection results from yolov8 so that the model can ground target object precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_1 = f\"\"\"\n",
    "You are an excellent AI agent who can read pictures and tell stories\n",
    "\n",
    "The group you are targeting are children and novices who don't understand anything\n",
    "\n",
    "please give me a output as a text format finally\n",
    "\n",
    "Remember:no less than 300 words\n",
    "\"\"\"\n",
    "\n",
    "prompt_2 = f\"\"\"\n",
    "\n",
    "You need to imagine a story based on the information in the image and tell it\n",
    "\n",
    "Include as much information as possible from all images\n",
    "\n",
    "###### USER QUERY ###### \n",
    "\n",
    "User query: {query}. You must return one answer.\n",
    "\n",
    "Please analyze this query and picture step by step, but there is no need to provide an analysis process\n",
    "###### OUTPUT ######\n",
    "Combining the above requirements,please use a paragraph to tell me a complete and interesting story.\n",
    "Answer: \n",
    "\"\"\"\n",
    "prompt = prompt_1 + prompt_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call LLM to reason\n",
    "The LLM api is called in format of openai. And the model we use here is the \"llava-7b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "  \n",
    "# 你需要传入进的图片路径\n",
    "image_path = \"temp\\\\out_for_llava.jpg\"\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"\")\n",
    "messages = []\n",
    "first_turn = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"{prompt}\",\n",
    "                },\n",
    "                {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "messages.append(first_turn)\n",
    "payload = {\n",
    "    \"model\": \"llava-v1.5-7b-llamafile\",\n",
    "    \"messages\": f\"{json.dumps(messages, indent=4)}\",\n",
    "}\n",
    "response = requests.post(\"http://127.0.0.1:1234/v1/chat/completions\", json=payload)\n",
    "print(response)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = str(response.json()['choices'][0]['message']['content'])\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trans to voice playback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字转语音的初始化设置\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate',150)\n",
    "engine.setProperty('volume',0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "engine.say(content)\n",
    "engine.runAndWait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ryzenai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
